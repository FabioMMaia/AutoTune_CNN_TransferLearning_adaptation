{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4DfzAgp/9bETS3e3VEdUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabioMMaia/AutoTune_CNN_TransferLearning_adaptation/blob/main/ajustado_AutoTune_VGG16_CompleteArc_WithFCLayers_Bayesian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.13.0"
      ],
      "metadata": {
        "id": "wam6pu2UzaeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2869eb7e-346c-4639-8ed4-dfc27d13eb11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed gast-0.4.0 keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPyOpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuH75epxp9W5",
        "outputId": "c7f524b1-ef1b-4e8d-edab-041cfa6b719a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPyOpt\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.11.3)\n",
            "Collecting GPy>=1.8 (from GPyOpt)\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.4/959.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (1.16.0)\n",
            "Collecting paramz>=0.9.0 (from GPy>=1.8->GPyOpt)\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (3.0.5)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.2)\n",
            "Building wheels for collected packages: GPyOpt, GPy, paramz\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83588 sha256=40cf824822a183cd23628a0e1699536d7599bcb3de8f0de0ae2766298f2ffa41\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/17/52/9d818b4c60f733bf49d5cf82bc2758ebbdc57a0471137c37be\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp310-cp310-linux_x86_64.whl size=3420842 sha256=84a8c60d22c99062e32b48dab76fae4f0fcefff3530eaab6e364fbef7dc1cdec\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/bd/9f/82ab4216eae088cba864ca0dc1d75699bd4bf6823790fb2f77\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102543 sha256=869b3df3c214eac88a74b7f0920d53bc2796c9c62a7fe43e23fd20228bbe5a23\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/ef/9c/da9ceef7d0ff5287c24365844fc394852c2b79ac3fcf33bf8b\n",
            "Successfully built GPyOpt GPy paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt\n",
            "Successfully installed GPy-1.10.0 GPyOpt-1.2.6 paramz-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import GPyOpt\n",
        "import keras\n",
        "import math\n",
        "from itertools import product\n",
        "from collections import OrderedDict\n",
        "from keras.preprocessing import image\n",
        "from keras import layers, models, optimizers, callbacks, initializers, activations\n",
        "from keras.applications import VGG16"
      ],
      "metadata": {
        "id": "6rEVIPUrmnQ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBu0fWz8m6ud",
        "outputId": "40bc9fb0-a57d-41dd-880b-6ea04117bd76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "dfTgmWeHmfnp"
      },
      "outputs": [],
      "source": [
        "reverse_list = lambda l: list(reversed(l))\n",
        "\n",
        "DATA_FOLDER = r'/content/drive/MyDrive/Mestrado/SIN5006/Apresentação II/Codigo/caltech-101 (1)'\n",
        "# DATA_FOLDER = \"CalTech101\"\n",
        "# TRAIN_PATH = os.path.join(DATA_FOLDER, \"training\") # Path for training data\n",
        "# VALID_PATH = os.path.join(DATA_FOLDER, \"validation\") # Path for validation data\n",
        "NUMBER_OF_CLASSES = 101 #len(os.listdir(TRAIN_PATH)) # Number of classes of the dataset\n",
        "EPOCHS = 50\n",
        "RESULTS_PATH = os.path.join(r'/content/drive/MyDrive/Mestrado/SIN5006/Apresentação II/Codigo/', \"AutoConv_VGG16_randomsearch_log\" + \"_autoconv_bayes_opt_vf.csv\") # The path to the results file\n",
        "# RESULTS_PATH = os.path.join(\"AutoConv_VGG16_new1\", \"Upsampling_AutoConv_VGG16_log_\" + DATA_FOLDER.split('/')[-1] + \"_autoconv_bayes_opt_v1.csv\") # The path to the results file\n",
        "\n",
        "# Creating generators from training and validation data\n",
        "batch_size=8 # the mini-batch size to use for the dataset\n",
        "# datagen = image.ImageDataGenerator(preprocessing_function=keras.applications.vgg16.preprocess_input) # creating an instance of the data generator\n",
        "# train_generator = datagen.flow_from_directory(TRAIN_PATH, target_size=(224, 224), batch_size=batch_size) # creating the generator for training data\n",
        "# valid_generator = datagen.flow_from_directory(VALID_PATH, target_size=(224, 224), batch_size=batch_size) # creating the generator for validation data\n",
        "\n",
        "# creating callbacks for the model\n",
        "\n",
        "reduce_LR = callbacks.ReduceLROnPlateau(monitor='accuracy', factor=np.sqrt(0.01), cooldown=0, patience=5, min_lr=0.5e-10)\n",
        "# reduce_LR = callbacks.ReduceLROnPlateau(monitor='!', factor=np.sqrt(0.01), cooldown=0, patience=5, min_lr=0.5e-10)\n",
        "\n",
        "NUM_HYPERPARAMS = 3\n",
        "\n",
        "# activations\n",
        "act_map = [\n",
        "    activations.relu,\n",
        "    activations.sigmoid,\n",
        "    activations.tanh,\n",
        "    activations.elu,\n",
        "    activations.selu\n",
        "]\n",
        "\n",
        "\n",
        "# Creating a CSV file if one does not exist\n",
        "try:\n",
        "    log_df = pd.read_csv(RESULTS_PATH, header=0, index_col=['index'])\n",
        "except FileNotFoundError:\n",
        "    log_df = pd.DataFrame(columns=['index', 'activation', 'weight_initializer', 'num_layers_tuned', 'num_fc_layers', 'num_neurons', 'dropouts', 'filter_sizes', 'num_filters', 'stride_sizes', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
        "    log_df = log_df.set_index('index')\n",
        "\n",
        "\n",
        "# utility function\n",
        "def upsample(shape, target_size=5):\n",
        "    upsampling_factor = math.ceil(target_size / shape[1])\n",
        "    return layers.UpSampling2D(size=(upsampling_factor, upsampling_factor))\n",
        "\n",
        "\n",
        "# function to modify architecture for current hyperparams\n",
        "def get_model_conv(model, index, architecture, conv_params, optim_neurons, optim_dropouts):\n",
        "    X = model.layers[index - 1].output\n",
        "\n",
        "    dense_params = OrderedDict(filter(lambda x: x[0].startswith('dense'), conv_params.items()))\n",
        "    conv_params = OrderedDict(filter(lambda x: not x[0].startswith('dense'), conv_params.items()))\n",
        "\n",
        "    for i in range(len(conv_params) // NUM_HYPERPARAMS):\n",
        "        global_index = index + i\n",
        "        if architecture[i] == 'add':\n",
        "            continue\n",
        "\n",
        "        params_dicts = OrderedDict(filter(lambda x: x[0].startswith(architecture[i]) and x[0].split('_')[-1] == str(-global_index), conv_params.items()))\n",
        "        filter_size, num_filters, stride_size = [x for x in params_dicts.values()]\n",
        "\n",
        "        if architecture[i] == 'conv':\n",
        "            assert type(model.layers[global_index]) == layers.Conv2D\n",
        "            try:\n",
        "                X = layers.Conv2D(filters=int(num_filters), kernel_size=(int(filter_size), int(filter_size)), kernel_initializer='he_normal', activation=act_map[int(stride_size)])(X)\n",
        "            except:\n",
        "                X = upsample(X.shape)(X)\n",
        "                X = layers.Conv2D(filters=int(num_filters), kernel_size=(int(filter_size), int(filter_size)), kernel_initializer='he_normal', activation=act_map[int(stride_size)])(X)\n",
        "        elif architecture[i] == 'maxpool':\n",
        "            assert type(model.layers[global_index]) == layers.MaxPooling2D\n",
        "            X = layers.MaxPooling2D(pool_size=int(filter_size))(X)\n",
        "        elif architecture[i] == 'globalavgpool':\n",
        "            assert type(model.layers[global_index]) == layers.GlobalAveragePooling2D\n",
        "            X = layers.GlobalAveragePooling2D()(X)\n",
        "        elif architecture[i] == 'batch':\n",
        "            assert type(model.layers[global_index]) == layers.BatchNormalization\n",
        "            X = layers.BatchNormalization()(X)\n",
        "        elif architecture[i] == 'activation':\n",
        "            assert type(model.layers[global_index]) == layers.Activation\n",
        "            X = layers.Activation(act_map[int(stride_size)])(X)\n",
        "        elif architecture[i] == 'flatten':\n",
        "            X = layers.Flatten()(X)\n",
        "\n",
        "    for i in range(len(dense_params) // NUM_HYPERPARAMS):\n",
        "        units = int(dense_params['dense_filter_size_' + str(i + 1)])\n",
        "        dropout = float(dense_params['dense_num_filters_' + str(i + 1)])\n",
        "        act = int(dense_params['dense_stride_size_' + str(i + 1)])\n",
        "\n",
        "        X = layers.Dense(units, activation=act_map[act], kernel_initializer='he_normal')(X)\n",
        "        X = layers.BatchNormalization()(X)\n",
        "        X = layers.Dropout(dropout)(X)\n",
        "\n",
        "    X = layers.Dense(NUMBER_OF_CLASSES, activation='softmax', kernel_initializer='he_normal')(X)\n",
        "    return models.Model(inputs=model.inputs, outputs=X)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Baixar o conjunto de dados Caltech101 do TensorFlow Datasets\n",
        "(train_data, valid_data), info = tfds.load(\n",
        "    'caltech101',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "# Configurar geradores de dados para treinamento e validação\n",
        "batch_size = 8\n",
        "target_size = (224, 224)\n",
        "\n",
        "def preprocess_data(image, label):\n",
        "    image = tf.image.resize(image, target_size)\n",
        "    image = preprocess_input(image)\n",
        "    label = tf.one_hot(label, 101)  # 101 classes no Caltech101\n",
        "    return image, label\n",
        "\n",
        "train_dataset = (\n",
        "    train_data\n",
        "    .map(preprocess_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    .shuffle(buffer_size=len(train_data))\n",
        "    .batch(batch_size)\n",
        "    .repeat()  # Adicione a função repeat para repetir o conjunto de dados indefinidamente\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    valid_data\n",
        "    .map(preprocess_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "dCsiJhCfm4Jx"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# training the original model\n",
        "base_model = VGG16(include_top=True, weights='imagenet', input_shape=(224, 224, 3))\n",
        "X = base_model.layers[-2].output\n",
        "X = layers.Dense(NUMBER_OF_CLASSES, activation='softmax', kernel_initializer='he_normal')(X)\n",
        "base_model = models.Model(inputs=base_model.inputs, outputs=X)\n",
        "\n",
        "# freezing the layers of the model\n",
        "for i in range(len(base_model.layers)-1):\n",
        "    base_model.layers[i].trainable = False\n",
        "\n",
        "base_model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Obter um número aproximado de amostras para cada conjunto\n",
        "# train_steps = int(np.ceil(len(train_images) / batch_size))\n",
        "# valid_steps = int(np.ceil(len(valid_images) / batch_size))\n",
        "\n",
        "steps_per_epoch = len(train_data) // batch_size\n",
        "\n",
        "\n",
        "history = base_model.fit_generator(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=len(valid_data) // batch_size,\n",
        "    callbacks=[reduce_LR]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCySa7XHmbSX",
        "outputId": "053e00a5-d177-4d13-8578-2f59550ee9d5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "306/306 [==============================] - 23s 66ms/step - loss: 3.2451 - accuracy: 0.3035 - val_loss: 1.7656 - val_accuracy: 0.5822 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "306/306 [==============================] - 21s 67ms/step - loss: 1.1572 - accuracy: 0.7341 - val_loss: 1.1717 - val_accuracy: 0.7220 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "306/306 [==============================] - 20s 67ms/step - loss: 0.7031 - accuracy: 0.8562 - val_loss: 0.9560 - val_accuracy: 0.7664 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.5009 - accuracy: 0.9040 - val_loss: 0.8484 - val_accuracy: 0.7878 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.3853 - accuracy: 0.9322 - val_loss: 0.7836 - val_accuracy: 0.8059 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "306/306 [==============================] - 20s 67ms/step - loss: 0.3108 - accuracy: 0.9473 - val_loss: 0.7414 - val_accuracy: 0.8109 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "306/306 [==============================] - 20s 67ms/step - loss: 0.2581 - accuracy: 0.9616 - val_loss: 0.7097 - val_accuracy: 0.8125 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.2195 - accuracy: 0.9722 - val_loss: 0.6882 - val_accuracy: 0.8174 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1897 - accuracy: 0.9771 - val_loss: 0.6689 - val_accuracy: 0.8257 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1663 - accuracy: 0.9812 - val_loss: 0.6545 - val_accuracy: 0.8273 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1477 - accuracy: 0.9841 - val_loss: 0.6424 - val_accuracy: 0.8289 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1324 - accuracy: 0.9853 - val_loss: 0.6310 - val_accuracy: 0.8289 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1199 - accuracy: 0.9869 - val_loss: 0.6208 - val_accuracy: 0.8289 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1092 - accuracy: 0.9882 - val_loss: 0.6133 - val_accuracy: 0.8306 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.1002 - accuracy: 0.9890 - val_loss: 0.6072 - val_accuracy: 0.8306 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0926 - accuracy: 0.9898 - val_loss: 0.6002 - val_accuracy: 0.8289 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0860 - accuracy: 0.9902 - val_loss: 0.5945 - val_accuracy: 0.8322 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0802 - accuracy: 0.9902 - val_loss: 0.5892 - val_accuracy: 0.8289 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0752 - accuracy: 0.9902 - val_loss: 0.5850 - val_accuracy: 0.8322 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0707 - accuracy: 0.9902 - val_loss: 0.5806 - val_accuracy: 0.8355 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0667 - accuracy: 0.9902 - val_loss: 0.5771 - val_accuracy: 0.8355 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0632 - accuracy: 0.9906 - val_loss: 0.5734 - val_accuracy: 0.8355 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0599 - accuracy: 0.9906 - val_loss: 0.5701 - val_accuracy: 0.8355 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0571 - accuracy: 0.9902 - val_loss: 0.5674 - val_accuracy: 0.8372 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0544 - accuracy: 0.9906 - val_loss: 0.5641 - val_accuracy: 0.8405 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0520 - accuracy: 0.9906 - val_loss: 0.5613 - val_accuracy: 0.8421 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0498 - accuracy: 0.9902 - val_loss: 0.5589 - val_accuracy: 0.8438 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0476 - accuracy: 0.9906 - val_loss: 0.5587 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0474 - accuracy: 0.9906 - val_loss: 0.5584 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0472 - accuracy: 0.9906 - val_loss: 0.5582 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 31/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0470 - accuracy: 0.9906 - val_loss: 0.5580 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 32/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0468 - accuracy: 0.9906 - val_loss: 0.5577 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 33/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5577 - val_accuracy: 0.8438 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5577 - val_accuracy: 0.8438 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5577 - val_accuracy: 0.8438 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 39/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 44/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 45/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 46/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 47/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 48/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-08\n",
            "Epoch 49/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-08\n",
            "Epoch 50/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.0465 - accuracy: 0.9906 - val_loss: 0.5576 - val_accuracy: 0.8438 - lr: 1.0000e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# log the results\n",
        "best_acc_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
        "assert history.history['val_accuracy'][best_acc_index] == max(history.history['val_accuracy'])\n",
        "log_tuple = ('relu', 'he_normal', 0, 1, [], [], [], [], [], history.history['loss'][best_acc_index],  history.history['accuracy'][best_acc_index], history.history['val_loss'][best_acc_index], history.history['val_accuracy'][best_acc_index])\n",
        "log_df.loc[log_df.shape[0], :] = log_tuple\n",
        "log_df.to_csv(RESULTS_PATH)\n",
        "\n",
        "# tuning the model\n",
        "base_model = VGG16(include_top=True, weights='imagenet', input_shape=(224, 224, 3))\n",
        "base_model = models.Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n",
        "for i in range(len(base_model.layers)):\n",
        "    base_model.layers[i].trainable = False\n",
        "\n",
        "\n",
        "# optimize layers\n",
        "optim_neurons = []\n",
        "optim_dropouts = []\n",
        "best_acc = 0\n",
        "# layers not considered in optimization\n",
        "meaningless = [\n",
        "    layers.Activation,\n",
        "    layers.GlobalAveragePooling2D,\n",
        "    layers.BatchNormalization,\n",
        "    layers.ZeroPadding2D,\n",
        "    layers.Add,\n",
        "    layers.Flatten\n",
        "]"
      ],
      "metadata": {
        "id": "kM3qJs2bszh8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, len(base_model.layers) + 1):\n",
        "    unfreeze = i\n",
        "    if type(base_model.layers[-i]) in meaningless:\n",
        "        continue\n",
        "    temp_model = models.Model(inputs=base_model.inputs, outputs=base_model.outputs)\n",
        "    print(f\"Tuning last {unfreeze} layers.\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    temp_arc = []\n",
        "    for j in range(1, unfreeze + 1):\n",
        "        if type(temp_model.layers[-j]) == layers.Conv2D:\n",
        "            temp_arc.append('conv')\n",
        "        elif type(temp_model.layers[-j]) == layers.MaxPooling2D:\n",
        "            temp_arc.append('maxpool')\n",
        "        elif type(temp_model.layers[-j]) == layers.GlobalAveragePooling2D:\n",
        "            temp_arc.append('globalavgpool')\n",
        "        elif type(temp_model.layers[-j]) == layers.Add:\n",
        "            temp_arc.append('add')\n",
        "        elif type(temp_model.layers[-j]) == layers.BatchNormalization:\n",
        "            temp_arc.append('batch')\n",
        "        elif type(temp_model.layers[-j]) == layers.ZeroPadding2D:\n",
        "            temp_arc.append('zeropad')\n",
        "        elif type(temp_model.layers[-j]) == layers.Dense:\n",
        "            temp_arc.append('dense')\n",
        "        elif type(temp_model.layers[-j]) == layers.Activation:\n",
        "            temp_arc.append('activation')\n",
        "        elif type(temp_model.layers[-j]) == layers.Flatten:\n",
        "            temp_arc.append('flatten')\n",
        "\n",
        "    # making bounds list\n",
        "    bounds = []\n",
        "    for iter_ in range(len(temp_arc)):\n",
        "        if temp_arc[iter_] == 'conv':\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': 'conv_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [2, 3, 5]},\n",
        "                    {'name': 'conv_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': [64, 128, 256, 512]},\n",
        "                    {'name': 'conv_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': list(range(len(act_map)))},\n",
        "                ]\n",
        "            )\n",
        "        elif temp_arc[iter_] == 'dense':\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': 'dense_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [2 ** j for j in range(6, 11)]},\n",
        "                    {'name': 'dense_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': np.arange(0, 1, step=0.1)},\n",
        "                    {'name': 'dense_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': list(range(len(act_map)))},\n",
        "                ]\n",
        "            )\n",
        "        elif temp_arc[iter_] == 'flatten':\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': 'flatten_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': 'flatten_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': 'flatten_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                ]\n",
        "            )\n",
        "        elif temp_arc[iter_] == 'maxpool':\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': 'maxpool_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [2, 3]},\n",
        "                    {'name': 'maxpool_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': 'maxpool_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                ]\n",
        "            )\n",
        "        elif temp_arc[iter_] == 'globalavgpool':\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': 'avgpool_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': 'avgpool_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': 'avgpool_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                ]\n",
        "            )\n",
        "        elif temp_arc[iter_] == 'activation':\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': 'activation_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': list(range(len(act_map)))},\n",
        "                    {'name': 'activation_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': 'activation_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            bounds.extend(\n",
        "                [\n",
        "                    {'name': temp_arc[iter_] + '_filter_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': temp_arc[iter_] + '_num_filters_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                    {'name': temp_arc[iter_] + '_stride_size_' + str(iter_ + 1), 'type': 'discrete', 'domain': [1]},\n",
        "                ]\n",
        "            )\n",
        "\n",
        "\n",
        "    history = None\n",
        "    def model_fit_conv(x):\n",
        "        \"\"\"\n",
        "        Callback function for GPyOpt optimizer\n",
        "        \"\"\"\n",
        "        global history\n",
        "\n",
        "        filter_sizes = []\n",
        "        num_filters = []\n",
        "        stride_sizes = []\n",
        "        acts = []\n",
        "\n",
        "        conv_params = OrderedDict()\n",
        "\n",
        "        j = 0\n",
        "        while j < x.shape[1]:\n",
        "            conv_params[temp_arc[j // NUM_HYPERPARAMS] + '_filter_size_' + str((j // NUM_HYPERPARAMS) + 1)] = x[:, j]\n",
        "            if temp_arc[j // NUM_HYPERPARAMS] not in meaningless:\n",
        "                filter_sizes.append(int(x[:, j]))\n",
        "            j += 1\n",
        "            conv_params[temp_arc[j // NUM_HYPERPARAMS] + '_num_filters_' + str((j // NUM_HYPERPARAMS) + 1)] = x[:, j]\n",
        "            if temp_arc[j // NUM_HYPERPARAMS] == 'conv':\n",
        "                num_filters.append(int(x[:, j]))\n",
        "            j += 1\n",
        "            conv_params[temp_arc[j // NUM_HYPERPARAMS] + '_stride_size_' + str((j // NUM_HYPERPARAMS) + 1)] = x[:, j]\n",
        "            if temp_arc[j // NUM_HYPERPARAMS] == 'conv' or temp_arc[j // NUM_HYPERPARAMS] == 'dense':\n",
        "                acts.append(act_map[int(x[:, j])])\n",
        "            j += 1\n",
        "\n",
        "        to_train_model = get_model_conv(temp_model, -len(conv_params) // NUM_HYPERPARAMS, reverse_list(temp_arc), conv_params, optim_neurons, optim_dropouts)\n",
        "        to_train_model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # train the modified model\n",
        "        history = to_train_model.fit_generator(\n",
        "            train_dataset,\n",
        "            validation_data=valid_dataset,\n",
        "            epochs=EPOCHS,\n",
        "            # steps_per_epoch=len(train_dataset) / batch_size,\n",
        "            # validation_steps=len(valid_dataset),\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_steps=len(valid_data) // batch_size,\n",
        "            callbacks=[reduce_LR]\n",
        "        )\n",
        "\n",
        "        best_acc_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
        "        assert history.history['val_accuracy'][best_acc_index] == max(history.history['val_accuracy'])\n",
        "        train_loss = history.history['loss'][best_acc_index]\n",
        "        train_acc = history.history['accuracy'][best_acc_index]\n",
        "        val_loss = history.history['val_loss'][best_acc_index]\n",
        "        val_acc = history.history['val_accuracy'][best_acc_index]\n",
        "\n",
        "        # log the results\n",
        "        log_tuple = (acts, 'he_normal', unfreeze, len(optim_neurons) + 1, optim_neurons, optim_dropouts, filter_sizes, num_filters, stride_sizes, train_loss, train_acc, val_loss, val_acc)\n",
        "        log_df.loc[log_df.shape[0], :] = log_tuple\n",
        "        log_df.to_csv(RESULTS_PATH)\n",
        "\n",
        "        return min(history.history['val_loss'])\n",
        "\n",
        "\n",
        "    # initialize the optimizer\n",
        "    opt_ = GPyOpt.methods.BayesianOptimization(f=model_fit_conv, domain=bounds)\n",
        "    opt_.run_optimization(max_iter=20)\n",
        "\n",
        "    best_acc_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
        "    assert history.history['val_accuracy'][best_acc_index] == max(history.history['val_accuracy'])\n",
        "    temp_acc = history.history['val_accuracy'][best_acc_index]\n",
        "\n",
        "    print(\"Optimized Parameters:\")\n",
        "    for k in range(len(bounds)):\n",
        "        print(f\"\\t{bounds[k]['name']}: {opt_.x_opt[k]}\")\n",
        "    print(f\"Optimized Function value: {opt_.fx_opt}\")\n",
        "\n",
        "    if temp_acc < best_acc:\n",
        "        print(\"Validation Accuracy did not improve\")\n",
        "        print(f\"Breaking out at {i} layers\")\n",
        "        break\n",
        "    best_acc = max(temp_acc, best_acc)\n",
        "\n",
        "    time.sleep(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I71Xqv2Op42j",
        "outputId": "e227711a-cd5e-48dc-d7cb-5ba37022b1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning last 1 layers.\n",
            "Epoch 1/50\n",
            "306/306 [==============================] - 23s 67ms/step - loss: 5.2182 - accuracy: 0.0670 - val_loss: 2.8938 - val_accuracy: 0.3701 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "306/306 [==============================] - 21s 67ms/step - loss: 3.3155 - accuracy: 0.2594 - val_loss: 2.2477 - val_accuracy: 0.5181 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 2.6180 - accuracy: 0.3672 - val_loss: 1.9078 - val_accuracy: 0.5987 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "306/306 [==============================] - 20s 65ms/step - loss: 2.2807 - accuracy: 0.4363 - val_loss: 1.6745 - val_accuracy: 0.6414 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.9545 - accuracy: 0.5078 - val_loss: 1.5337 - val_accuracy: 0.6546 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.8231 - accuracy: 0.5507 - val_loss: 1.4359 - val_accuracy: 0.6941 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.6381 - accuracy: 0.5874 - val_loss: 1.3385 - val_accuracy: 0.7204 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.4840 - accuracy: 0.6352 - val_loss: 1.2639 - val_accuracy: 0.7270 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.4241 - accuracy: 0.6467 - val_loss: 1.2188 - val_accuracy: 0.7401 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.3196 - accuracy: 0.6646 - val_loss: 1.1736 - val_accuracy: 0.7434 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.2768 - accuracy: 0.6842 - val_loss: 1.1329 - val_accuracy: 0.7632 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.1762 - accuracy: 0.7067 - val_loss: 1.1066 - val_accuracy: 0.7730 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.0842 - accuracy: 0.7402 - val_loss: 1.0691 - val_accuracy: 0.7780 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.0396 - accuracy: 0.7382 - val_loss: 1.0459 - val_accuracy: 0.7812 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 1.0402 - accuracy: 0.7447 - val_loss: 1.0265 - val_accuracy: 0.7829 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.9949 - accuracy: 0.7533 - val_loss: 0.9992 - val_accuracy: 0.7944 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.9424 - accuracy: 0.7761 - val_loss: 0.9798 - val_accuracy: 0.7961 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.8780 - accuracy: 0.7786 - val_loss: 0.9627 - val_accuracy: 0.7977 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.8442 - accuracy: 0.7888 - val_loss: 0.9472 - val_accuracy: 0.7944 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.8274 - accuracy: 0.8015 - val_loss: 0.9405 - val_accuracy: 0.7961 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "306/306 [==============================] - 20s 66ms/step - loss: 0.7892 - accuracy: 0.8133 - val_loss: 0.9076 - val_accuracy: 0.7993 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "112/306 [=========>....................] - ETA: 10s - loss: 0.7979 - accuracy: 0.8058"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLlL5rA_uygY",
        "outputId": "b30381d0-e98a-4673-c61e-249bdd4cb94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0330033004283905"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPhAQGbs-ZLO",
        "outputId": "7529f8a8-4631-4ff3-c0b3-65d1e5f66ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19801980257034302"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# examinando a rede alterada"
      ],
      "metadata": {
        "id": "1VcM6_RmPtai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_strucuture():\n",
        "\n",
        "  def get_model_conv(model,\n",
        "                   index,\n",
        "                   architecture,\n",
        "                   num_filters,\n",
        "                   filter_sizes,\n",
        "                   pool_sizes,\n",
        "                   acts,\n",
        "                   zero_pads,\n",
        "                   optim_neurons,\n",
        "                   optim_dropouts):\n",
        "\n",
        "    X = model.layers[index - 1].output\n",
        "\n",
        "    for i in range(len(architecture)):\n",
        "        global_index = index + i\n",
        "        if architecture[i] == 'add':\n",
        "            continue\n",
        "\n",
        "        if architecture[i] == 'conv':\n",
        "            assert type(model.layers[global_index]) == layers.Conv2D\n",
        "            num_filter = num_filters.pop(0)\n",
        "            filter_size = filter_sizes.pop(0)\n",
        "            act = acts.pop(0)\n",
        "            try:\n",
        "                X = layers.Conv2D(filters=int(num_filter), kernel_size=(int(filter_size), int(filter_size)), kernel_initializer='he_normal', activation=act)(X)\n",
        "            except:\n",
        "                X = upsample(X.shape)(X)\n",
        "                X = layers.Conv2D(filters=int(num_filter), kernel_size=(int(filter_size), int(filter_size)), kernel_initializer='he_normal', activation=act)(X)\n",
        "        elif architecture[i] == 'maxpool':\n",
        "            assert type(model.layers[global_index]) == layers.MaxPooling2D\n",
        "            pool_size = pool_sizes.pop(0)\n",
        "            X = layers.MaxPooling2D(pool_size=int(pool_size))(X)\n",
        "        elif architecture[i] == 'globalavgpool':\n",
        "            assert type(model.layers[global_index]) == layers.GlobalAveragePooling2D\n",
        "            X = layers.GlobalAveragePooling2D()(X)\n",
        "        elif architecture[i] == 'batch':\n",
        "            assert type(model.layers[global_index]) == layers.BatchNormalization\n",
        "            X = layers.BatchNormalization()(X)\n",
        "        elif architecture[i] == 'activation':\n",
        "            assert type(model.layers[global_index]) == layers.Activation\n",
        "            X = layers.Activation(acts.pop(0))(X)\n",
        "        elif architecture[i] == 'flatten':\n",
        "            X = layers.Flatten()(X)\n",
        "\n",
        "    for units, dropout in zip(optim_neurons, optim_dropouts):\n",
        "        X = layers.Dense(units, kernel_initializer='he_normal', activation=acts.pop(0))(X)\n",
        "        X = layers.BatchNormalization()(X)\n",
        "        X = layers.Dropout(float(dropout))(X)\n",
        "\n",
        "    X = layers.Dense(NUMBER_OF_CLASSES, activation='softmax', kernel_initializer='he_normal')(X)\n",
        "    return models.Model(inputs=model.inputs, outputs=X)\n",
        "\n",
        "  # optimize layers\n",
        "  best_acc = 0\n",
        "\n",
        "  # list of layers not considered in optimization\n",
        "  meaningless = [\n",
        "      layers.Activation,\n",
        "      layers.GlobalAveragePooling2D,\n",
        "      layers.ZeroPadding2D,\n",
        "      layers.Add,\n",
        "      layers.Flatten\n",
        "  ]\n",
        "\n",
        "  # search spaces for each kind of hyperparam\n",
        "  filter_size_space = [1, 3]\n",
        "  num_filter_space = [32, 64, 128, 256]\n",
        "  pool_size_space = [2, 3]\n",
        "  units_space = [2 ** j for j in range(6, 11)]\n",
        "  dropouts_space = np.arange(0, 1, step=0.1).tolist()\n",
        "  pad_size_space = list(range(1, 5))\n",
        "  acts_space = [\n",
        "      activations.relu,\n",
        "      activations.sigmoid,\n",
        "      activations.tanh,\n",
        "      activations.elu,\n",
        "      activations.selu\n",
        "  ]\n",
        "\n",
        "\n",
        "  import random\n",
        "  for unfreeze in range(1, len(base_model.layers) + 1):\n",
        "      print(f\"Tuning last {unfreeze} layers.\")\n",
        "      if type(base_model.layers[-unfreeze]) in meaningless:\n",
        "          continue\n",
        "\n",
        "      iter_accs = []\n",
        "\n",
        "      for k in range(20):\n",
        "          temp_model = models.Model(inputs=base_model.inputs, outputs=base_model.outputs)\n",
        "          time.sleep(3)\n",
        "\n",
        "          curr_filter_size = []\n",
        "          curr_num_filters = []\n",
        "          curr_pool_size = []\n",
        "          curr_acts = []\n",
        "          curr_pad = []\n",
        "          curr_units = []\n",
        "          curr_dropouts = []\n",
        "\n",
        "          # saving the architecture\n",
        "          temp_arc = []\n",
        "          for j in range(1, unfreeze + 1):\n",
        "              if type(temp_model.layers[-j]) == layers.Conv2D:\n",
        "                  temp_arc.append('conv')\n",
        "                  curr_filter_size.append(random.sample(filter_size_space, 1)[0])\n",
        "                  curr_num_filters.append(random.sample(num_filter_space, 1)[0])\n",
        "                  curr_acts.append(random.sample(acts_space, 1)[0])\n",
        "              elif type(temp_model.layers[-j]) == layers.MaxPooling2D:\n",
        "                  temp_arc.append('maxpool')\n",
        "                  curr_pool_size.append(random.sample(pool_size_space, 1)[0])\n",
        "              elif type(temp_model.layers[-j]) == layers.GlobalAveragePooling2D:\n",
        "                  temp_arc.append('globalavgpool')\n",
        "              elif type(temp_model.layers[-j]) == layers.Activation:\n",
        "                  temp_arc.append('activation')\n",
        "                  curr_acts.append(random.sample(acts_space, 1)[0])\n",
        "              elif type(temp_model.layers[-j]) == layers.Add:\n",
        "                  temp_arc.append('add')\n",
        "              elif type(temp_model.layers[-j]) == layers.BatchNormalization:\n",
        "                  temp_arc.append('batch')\n",
        "              elif type(temp_model.layers[-j]) == layers.ZeroPadding2D:\n",
        "                  temp_arc.append('zeropad')\n",
        "                  curr_pad.append(random.sample(pad_size_space, 1)[0])\n",
        "              elif type(temp_model.layers[-j]) == layers.Dense:\n",
        "                  temp_arc.append('dense')\n",
        "                  curr_units.append(random.sample(units_space, 1)[0])\n",
        "                  curr_dropouts.append(random.sample(dropouts_space, 1)[0])\n",
        "                  curr_acts.append(random.sample(acts_space, 1)[0])\n",
        "              elif type(temp_model.layers[-j]) == layers.Flatten:\n",
        "                  temp_arc.append('flatten')\n",
        "\n",
        "          # for each iteration - create another model\n",
        "          print(f'#{k}')\n",
        "          to_train_model = get_model_conv(temp_model, # temporary model\n",
        "                                          -unfreeze, # number of layer to be modified\n",
        "                                          reverse_list(temp_arc),\n",
        "                                          reverse_list(curr_num_filters),\n",
        "                                          reverse_list(curr_filter_size),\n",
        "                                          reverse_list(curr_pool_size),\n",
        "                                          reverse_list(curr_acts), # acts are parameters to modify the actvation target layer\n",
        "                                          reverse_list(curr_pad),\n",
        "                                          curr_units,\n",
        "                                          curr_dropouts)\n",
        "\n",
        "          return to_train_model\n",
        "\n",
        "aux_model = test_strucuture()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGzcfNVDPwaf",
        "outputId": "97e40b77-c6ab-4daa-b5f7-e8951e1064c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning last 1 layers.\n",
            "#0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aux_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-ID1ySn-bJc",
        "outputId": "6c783f29-099c-4d0a-a6d0-58365d39de3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 1024)              4195328   \n",
            "                                                                 \n",
            " batch_normalization_59 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 101)               103525    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 121782181 (464.56 MB)\n",
            "Trainable params: 4300901 (16.41 MB)\n",
            "Non-trainable params: 117481280 (448.16 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_O5XNgQqvf",
        "outputId": "77c88dcf-845a-4f16-d358-29501d8228f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134260544 (512.16 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 134260544 (512.16 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHp99-zn-gP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}